"""
ç‹¬ç«‹æµè§ˆå™¨æŠ“å–æœåŠ¡

ä¸€ä¸ªæ”¯æŒé«˜å¹¶å‘çš„ç½‘é¡µæŠ“å–æœåŠ¡ï¼Œä½¿ç”¨ Playwright å®ç°ã€‚
æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹ï¼Œç¡®ä¿çœŸæ­£çš„å¹¶å‘å¤„ç†ã€‚
"""
import asyncio
import logging
import os
import random
import re
import time
import psutil
from contextlib import asynccontextmanager
from typing import Any
from urllib.parse import urljoin

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from playwright.async_api import async_playwright, Browser, async_playwright
from playwright_stealth import Stealth
from markdownify import markdownify

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# ==================== é…ç½® ====================

class Config:
    """æœåŠ¡é…ç½®"""

    # æœåŠ¡é…ç½®
    PORT = int(os.getenv('BROWSER_SERVICE_PORT', '2025'))
    HOST = os.getenv('BROWSER_SERVICE_HOST', '0.0.0.0')

    # æµè§ˆå™¨é…ç½®
    POOL_SIZE = int(os.getenv('BROWSER_POOL_SIZE', '3'))  # å‡å°‘ï¼š5->3ï¼Œæ¯ä¸ªæµè§ˆå™¨çº¦ 200-400MB
    MAX_CONCURRENT_PAGES = int(os.getenv('MAX_CONCURRENT_PAGES', '10'))
    HEADLESS = os.getenv('HEADLESS', 'true').lower() == 'true'
    MAX_SCREENSHOT_SIZE = int(os.getenv('MAX_SCREENSHOT_SIZE', '5242880'))

    # User-Agent æ± 
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]

    # æµè§ˆå™¨å¯åŠ¨å‚æ•°
    BROWSER_ARGS = [
        '--no-sandbox',
        '--disable-dev-shm-usage',
        '--disable-blink-features=AutomationControlled',
        '--disable-gpu',
        '--disable-extensions',
        '--disable-background-networking',
        '--disable-default-apps',
        '--disable-sync',
        '--no-first-run',
        '--disable-setuid-sandbox',
        # å†…å­˜ä¼˜åŒ–å‚æ•°
        '--disable-background-timer-throttling',
        '--disable-backgrounding-occluded-windows',
        '--disable-breakpad',
        '--disable-client-side-phishing-detection',
        '--disable-component-extensions-with-background-pages',
        '--disable-features=TranslateUI,VizDisplayCompositor',
        '--disable-hang-monitor',
        '--disable-ipc-flooding-protection',
        '--disable-renderer-backgrounding',
        '--disable-features=site-per-process',
        '--disable-leak-detection',
    ]

    @classmethod
    def get_random_user_agent(cls) -> str:
        """è·å–éšæœº User-Agent"""
        return random.choice(cls.USER_AGENTS)


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class FetchRequest(BaseModel):
    """æŠ“å–è¯·æ±‚"""
    url: str
    wait_time: int = 200  # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    wait_for_selector: str = ""  # ç­‰å¾…é€‰æ‹©å™¨
    screenshot: bool = True  # æ˜¯å¦æˆªå›¾
    block_media: bool = True  # æ˜¯å¦é˜»æ­¢å›¾ç‰‡/è§†é¢‘åŠ è½½ï¼ˆé™ä½å†…å­˜ï¼‰


class FetchResponse(BaseModel):
    """æŠ“å–å“åº”"""
    success: bool
    fetched_url: str
    title: str = ""
    content: str = ""
    screenshot: str = ""  # base64 ç¼–ç 
    content_length: int = 0
    fetched_at: str = ""
    error: str = ""
    duration_seconds: float = 0  # æŠ“å–è€—æ—¶ï¼ˆç§’ï¼‰


# ==================== å†…å­˜ç›‘æ§å·¥å…· ====================

def get_memory_info() -> dict[str, Any]:
    """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä¿¡æ¯"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()

    # è·å–æ‰€æœ‰å­è¿›ç¨‹ï¼ˆChromium è¿›ç¨‹ï¼‰
    children = process.children(recursive=True)
    children_mem = 0
    chromium_count = 0
    chromium_details = []  # æ¯ä¸ª Chromium è¿›ç¨‹çš„è¯¦ç»†ä¿¡æ¯

    for child in children:
        try:
            child_mem = child.memory_info().rss
            children_mem += child_mem
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Chromium è¿›ç¨‹
            if 'chrom' in child.name().lower() or 'chrome' in child.name().lower():
                chromium_count += 1
                chromium_details.append({
                    "pid": child.pid,
                    "name": child.name(),
                    "rss_mb": round(child_mem / 1024 / 1024, 2),
                })
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass

    return {
        "process_rss_mb": round(mem_info.rss / 1024 / 1024, 2),
        "process_vms_mb": round(mem_info.vms / 1024 / 1024, 2),
        "children_rss_mb": round(children_mem / 1024 / 1024, 2),
        "total_rss_mb": round((mem_info.rss + children_mem) / 1024 / 1024, 2),
        "chromium_processes": chromium_count,
        "total_children": len(children),
        "chromium_details": chromium_details,  # æ¯ä¸ª Chromium è¿›ç¨‹çš„è¯¦ç»†ä¿¡æ¯
    }


def format_bytes(bytes_value: int) -> str:
    """æ ¼å¼åŒ–å­—èŠ‚æ•°ä¸ºå¯è¯»æ ¼å¼"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_value < 1024.0:
            return f"{bytes_value:.2f} {unit}"
        bytes_value /= 1024.0
    return f"{bytes_value:.2f} TB"


# ==================== æµè§ˆå™¨å®ä¾‹æ±  ====================

class BrowserPool:
    """æµè§ˆå™¨å®ä¾‹æ± """

    def __init__(self, pool_size: int):
        self.pool_size = pool_size
        self.browsers: list[Browser] = []
        self.playwright = None
        self.semaphore = asyncio.Semaphore(pool_size)
        self._initialized = False
        self._request_count = 0  # è¯·æ±‚è®¡æ•°å™¨
        self._start_time = time.time()  # å¯åŠ¨æ—¶é—´
        self._stealth = Stealth()  # å¤ç”¨ Stealth å®ä¾‹
        self._fetch_counts = [0] * pool_size  # æ¯ä¸ªæµè§ˆå™¨çš„æŠ“å–è®¡æ•°
        self._restart_threshold = 10  # æ¯æŠ“å– 10 æ¬¡å¼ºåˆ¶é‡å¯
        self._last_used: list = [0.0] * pool_size  # æ¯ä¸ªæµè§ˆå™¨çš„æœ€åä½¿ç”¨æ—¶é—´
        self._idle_timeout = 5  # ç©ºé—² 5 ç§’åé‡å¯ï¼ˆå¦‚æœæœ‰ä½¿ç”¨è¿‡ï¼‰

    async def initialize(self):
        """åˆå§‹åŒ–æµè§ˆå™¨æ± """
        if self._initialized:
            return

        logger.info(f"åˆå§‹åŒ–æµè§ˆå™¨å®ä¾‹æ± ï¼Œå¤§å°: {self.pool_size}")

        try:
            self.playwright = await async_playwright().start()

            # å¯åŠ¨å¤šä¸ªæµè§ˆå™¨å®ä¾‹
            for i in range(self.pool_size):
                browser = await self.playwright.chromium.launch(
                    headless=Config.HEADLESS,
                    args=Config.BROWSER_ARGS
                )
                self.browsers.append(browser)
                logger.info(f"æµè§ˆå™¨å®ä¾‹ {i}: å·²å¯åŠ¨")

            self._initialized = True
            logger.info(f"æµè§ˆå™¨å®ä¾‹æ± åˆå§‹åŒ–å®Œæˆï¼Œå®ä¾‹æ•°: {len(self.browsers)}")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æµè§ˆå™¨æ± å¤±è´¥: {e}")
            raise

    async def shutdown(self):
        """å…³é—­æ‰€æœ‰æµè§ˆå™¨å®ä¾‹"""
        logger.info("å…³é—­æµè§ˆå™¨å®ä¾‹æ± ...")

        for i, browser in enumerate(self.browsers):
            try:
                await browser.close()
                logger.info(f"æµè§ˆå™¨å®ä¾‹ {i}: å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­æµè§ˆå™¨å®ä¾‹ {i} æ—¶å‡ºé”™: {e}")

        self.browsers.clear()

        if self.playwright:
            try:
                await self.playwright.stop()
                logger.info("Playwright å·²åœæ­¢")
            except Exception as e:
                logger.warning(f"åœæ­¢ Playwright æ—¶å‡ºé”™: {e}")

        self._initialized = False

    async def fetch_page(self, request: FetchRequest) -> FetchResponse:
        """ä»æ± ä¸­è·å–ä¸€ä¸ªæµè§ˆå™¨å®ä¾‹æ¥æŠ“å–é¡µé¢"""
        if not self._initialized:
            await self.initialize()

        self._request_count += 1
        start_time = time.time()

        # å†…å­˜ç›‘æ§ä»»åŠ¡
        monitor_task = None
        stop_monitor = asyncio.Event()

        async def monitor_memory():
            """å¼‚æ­¥ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
            while not stop_monitor.is_set():
                mem_info = get_memory_info()
                logger.info(
                    f"ğŸ“Š [æŠ“å–ä¸­] RSS: {mem_info['process_rss_mb']:.1f}MB | "
                    f"å­è¿›ç¨‹: {mem_info['children_rss_mb']:.1f}MB | "
                    f"æ€»è®¡: {mem_info['total_rss_mb']:.1f}MB"
                )
                # æ˜¾ç¤ºæ¯ä¸ª Chromium è¿›ç¨‹çš„å†…å­˜
                if mem_info['chromium_details']:
                    for detail in mem_info['chromium_details']:
                        logger.info(f"  â””â”€ PID {detail['pid']} ({detail['name']}): {detail['rss_mb']:.1f}MB")
                try:
                    await asyncio.wait_for(stop_monitor.wait(), timeout=2.0)
                except asyncio.TimeoutError:
                    continue

        async with self.semaphore:
            # è·å–ä¸€ä¸ªå¯ç”¨çš„æµè§ˆå™¨å®ä¾‹ï¼ˆè½®è¯¢ï¼‰
            browser_index = id(asyncio.current_task()) % len(self.browsers)
            browser = self.browsers[browser_index]

            context = None
            page = None

            try:
                # å¯åŠ¨å†…å­˜ç›‘æ§
                monitor_task = asyncio.create_task(monitor_memory())

                # æ›´æ–°å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—ç©ºé—²ï¼‰
                self._last_used[browser_index] = time.time()

                # æ¯æ¬¡åˆ›å»ºæ–°çš„ contextï¼ˆå¹²å‡€éš”ç¦»ï¼Œåˆ›å»ºå¾ˆå¿«ï¼‰
                context = await browser.new_context(
                    viewport={"width": 1280, "height": 720},
                    user_agent=Config.get_random_user_agent(),
                )

                page = await context.new_page()

                # åªæ‹¦æˆªçœŸæ­£çš„åª’ä½“æ–‡ä»¶ï¼Œä¸é˜»æ­¢æ ·å¼å’Œå­—ä½“
                if request.block_media:
                    async def block_media_route(route, request):
                        resource_type = request.resource_type
                        # åªé˜»æ­¢å›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ï¼Œå…è®¸æ‰€æœ‰å…¶ä»–èµ„æº
                        if resource_type in ["image", "media", "audio", "video"]:
                            await route.abort()
                        else:
                            await route.continue_()
                    await page.route("**", block_media_route)

                # åº”ç”¨åçˆ¬è™«è„šæœ¬
                await self._apply_stealth(page)

                # è®¾ç½®è¯·æ±‚å¤´
                await page.set_extra_http_headers(self._get_headers())

                # å¯¼èˆªåˆ°é¡µé¢ï¼Œç­‰å¾…å®Œå…¨åŠ è½½ï¼ˆè¶…æ—¶åˆ™ä½¿ç”¨å·²åŠ è½½å†…å®¹ï¼‰
                try:
                    await page.goto(request.url, wait_until="load", timeout=30000)
                except Exception as goto_error:
                    logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶æˆ–å‡ºé”™ï¼Œä½¿ç”¨å·²åŠ è½½å†…å®¹: {goto_error}")

                # ç­‰å¾…æŒ‡å®šæ—¶é—´
                if request.wait_time > 0:
                    await page.wait_for_timeout(request.wait_time)

                # ç­‰å¾…é€‰æ‹©å™¨ï¼ˆè¶…æ—¶ä¸å½±å“ç»“æœï¼‰
                if request.wait_for_selector:
                    try:
                        await page.wait_for_selector(request.wait_for_selector, timeout=10000)
                    except Exception:
                        logger.warning(f"ç­‰å¾…é€‰æ‹©å™¨è¶…æ—¶: {request.wait_for_selector}")

                # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
                await self._scroll_page(page)

                # è·å–å†…å®¹
                title = await page.title()
                html_content = await page.content()

                # å¼‚æ­¥è½¬æ¢ä¸º Markdownï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
                markdown_content = await asyncio.to_thread(markdownify, html_content)
                cleaned_content = self._clean_markdown(markdown_content)
                # ä¿®å¤ç›¸å¯¹é“¾æ¥ä¸ºç»å¯¹é“¾æ¥
                fixed_content = self._fix_links(cleaned_content, request.url)

                # æˆªå›¾ï¼ˆæ•´é¡µï¼ŒJPEG æ ¼å¼é™ä½è´¨é‡ä»¥å‡å°æ–‡ä»¶å¤§å°ï¼‰
                screenshot_b64 = ""
                if request.screenshot:
                    import base64
                    screenshot_bytes = await page.screenshot(
                        full_page=True,
                        type="jpeg",
                        quality=60  # JPEG è´¨é‡ 0-100ï¼Œ60 å¹³è¡¡è´¨é‡å’Œå¤§å°
                    )
                    screenshot_b64 = base64.b64encode(screenshot_bytes).decode()

                duration_seconds = time.time() - start_time

                return FetchResponse(
                    success=True,
                    fetched_url=request.url,
                    title=title or "æ— æ ‡é¢˜",
                    content=fixed_content,
                    screenshot=screenshot_b64,
                    content_length=len(fixed_content),
                    fetched_at=time.strftime("%Y-%m-%d %H:%M:%S"),
                    duration_seconds=duration_seconds
                )

            except Exception as e:
                logger.error(f"æŠ“å–å¤±è´¥ {request.url}: {e}")
                duration_seconds = time.time() - start_time
                return FetchResponse(
                    success=False,
                    fetched_url=request.url,
                    error=str(e),
                    duration_seconds=duration_seconds
                )

            finally:
                # åœæ­¢å†…å­˜ç›‘æ§
                stop_monitor.set()
                if monitor_task:
                    try:
                        await asyncio.wait_for(monitor_task, timeout=0.5)
                    except (asyncio.TimeoutError, asyncio.CancelledError):
                        pass

                # å…³é—­é¡µé¢å’Œ contextï¼Œå½»åº•é‡Šæ”¾å†…å­˜
                if page:
                    try:
                        await page.evaluate("window.document.body.innerHTML = ''")
                        await page.close()
                        page = None
                    except:
                        page = None

                if context:
                    try:
                        await context.close()
                        context = None
                    except:
                        context = None

                # å¼ºåˆ¶å¤šæ¬¡åƒåœ¾å›æ”¶ï¼Œç¡®ä¿å†…å­˜é‡Šæ”¾
                import gc
                for _ in range(3):
                    gc.collect()

                # è¯·æ±‚å®Œæˆåçš„å†…å­˜çŠ¶æ€
                mem_info = get_memory_info()
                logger.info(
                    f"ğŸ“Š [æŠ“å–å®Œæˆ] RSS: {mem_info['process_rss_mb']:.1f}MB | "
                    f"å­è¿›ç¨‹: {mem_info['children_rss_mb']:.1f}MB | "
                    f"æ€»è®¡: {mem_info['total_rss_mb']:.1f}MB"
                )

                # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                self._last_used[browser_index] = time.time()
                # æ˜¾ç¤ºæ¯ä¸ª Chromium è¿›ç¨‹çš„å†…å­˜
                if mem_info['chromium_details']:
                    for detail in mem_info['chromium_details']:
                        logger.info(f"  â””â”€ PID {detail['pid']} ({detail['name']}): {detail['rss_mb']:.1f}MB")

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¯æµè§ˆå™¨
                self._fetch_counts[browser_index] += 1

                # è®¡ç®—ç©ºé—²æ—¶é—´
                idle_time = time.time() - self._last_used[browser_index]
                has_been_used = self._fetch_counts[browser_index] > 0

                # é‡å¯æ¡ä»¶ï¼šè¾¾åˆ°10æ¬¡ æˆ– (æœ‰ä½¿ç”¨è¿‡ä¸”ç©ºé—²è¶…è¿‡5ç§’)
                should_restart = (
                    self._fetch_counts[browser_index] >= self._restart_threshold or
                    (has_been_used and idle_time > self._idle_timeout)
                )

                if should_restart:
                    reason = "è¾¾åˆ°10æ¬¡" if self._fetch_counts[browser_index] >= self._restart_threshold else f"ç©ºé—²{idle_time:.0f}ç§’"
                    logger.info(f"æµè§ˆå™¨ {browser_index} {reason}ï¼Œæ‰§è¡Œé‡å¯...")
                    self._fetch_counts[browser_index] = 0
                    try:
                        await browser.close()
                        new_browser = await self.playwright.chromium.launch(
                            headless=Config.HEADLESS,
                            args=Config.BROWSER_ARGS
                        )
                        self.browsers[browser_index] = new_browser

                        # é‡å¯åçš„å†…å­˜çŠ¶æ€
                        import gc
                        gc.collect()
                        mem_info = get_memory_info()
                        logger.info(
                            f"ğŸ“Š [é‡å¯å®Œæˆ] RSS: {mem_info['process_rss_mb']:.1f}MB | "
                            f"å­è¿›ç¨‹: {mem_info['children_rss_mb']:.1f}MB | "
                            f"æ€»è®¡: {mem_info['total_rss_mb']:.1f}MB"
                        )
                        if mem_info['chromium_details']:
                            for detail in mem_info['chromium_details']:
                                logger.info(f"  â””â”€ PID {detail['pid']} ({detail['name']}): {detail['rss_mb']:.1f}MB")
                    except Exception as e:
                        logger.error(f"é‡å¯æµè§ˆå™¨ {browser_index} å¤±è´¥: {e}")

    async def _apply_stealth(self, page):
        """åº”ç”¨åçˆ¬è™«è„šæœ¬"""
        await self._stealth.apply_stealth_async(page)

    def _get_headers(self) -> dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "DNT": "1",
        }

    async def _scroll_page(self, page) -> None:
        """æ™ºèƒ½æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ‡’åŠ è½½å†…å®¹

        Args:
            page: Playwright é¡µé¢å¯¹è±¡

        Returns:
            None
        """
        max_scrolls = 20
        scroll_wait_ms = 500

        try:
            for i in range(max_scrolls):
                # æ£€æŸ¥æ˜¯å¦å·²æ»šåŠ¨åˆ°åº•éƒ¨
                is_at_bottom = await page.evaluate("""
                    () => {
                        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
                        const windowHeight = window.innerHeight || document.documentElement.clientHeight;
                        const documentHeight = document.documentElement.scrollHeight;
                        return scrollTop + windowHeight >= documentHeight - 100;
                    }
                """)

                if is_at_bottom:
                    logger.info(f"å·²æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œç¬¬ {i+1} æ¬¡")
                    break

                # æ‰§è¡Œæ»šåŠ¨
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(scroll_wait_ms / 1000)

                logger.debug(f"æ‰§è¡Œç¬¬ {i+1} æ¬¡æ»šåŠ¨")

        except Exception as e:
            logger.warning(f"æ»šåŠ¨è¿‡ç¨‹å‡ºé”™: {e}")

    def _clean_markdown(self, content: str) -> str:
        """æ¸…ç† Markdown"""
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r'^\s+$/gm', '', content)
        return content.strip()

    def _fix_links(self, content: str, base_url: str) -> str:
        """ä¿®å¤ Markdown ä¸­çš„ç›¸å¯¹é“¾æ¥ä¸ºç»å¯¹é“¾æ¥

        Args:
            content: Markdown å†…å®¹
            base_url: åŸºç¡€ URL

        Returns:
            ä¿®å¤åçš„ Markdown å†…å®¹
        """
        # æå–åŸºç¡€ URL çš„åè®®ï¼ˆhttp æˆ– httpsï¼‰
        base_protocol = 'https://' if base_url.startswith('https://') else 'http://'

        # ä¿®å¤ Markdown é“¾æ¥è¯­æ³• [æ–‡æœ¬](é“¾æ¥) å’Œå›¾ç‰‡ ![alt](url)
        def fix_markdown_link(match):
            is_image = match.group(1).startswith('!')  # æ˜¯å¦æ˜¯å›¾ç‰‡
            text = match.group(2)
            url = match.group(3)
            # è·³è¿‡å·²ç»æ˜¯ç»å¯¹é“¾æ¥çš„
            if url.startswith(('http://', 'https://', '#', 'mailto:', 'tel:')):
                return match.group(0)
            # å¤„ç†åè®®ç›¸å¯¹é“¾æ¥ //example.com
            if url.startswith('//'):
                url = base_protocol + url
                return match.group(0).replace(f']({match.group(3)})', f']({url})')
            # è½¬æ¢ä¸ºç»å¯¹é“¾æ¥
            absolute_url = urljoin(base_url, url)
            return match.group(0).replace(f']({match.group(3)})', f']({absolute_url})')

        # åŒ¹é… [text](url) å’Œ ![alt](url)
        content = re.sub(r'(\!?\[)([^\]]+)\]\(([^)]+)\)', fix_markdown_link, content)

        # ä¿®å¤ HTML æ ‡ç­¾ä¸­çš„é“¾æ¥
        def fix_html_link(match):
            tag = match.group(1)
            url = match.group(2)
            # ç§»é™¤ JavaScript é“¾æ¥ï¼ˆå®‰å…¨è€ƒè™‘ï¼‰
            if url.startswith('javascript:'):
                return f'{tag}="#"'
            # è·³è¿‡å·²ç»æ˜¯ç»å¯¹é“¾æ¥çš„
            if url.startswith(('http://', 'https://', '#', 'mailto:', 'tel:', 'data:')):
                return match.group(0)
            # å¤„ç†åè®®ç›¸å¯¹é“¾æ¥ //example.com
            if url.startswith('//'):
                absolute_url = base_protocol + url
                return f'{tag}="{absolute_url}"'
            # è½¬æ¢ä¸ºç»å¯¹é“¾æ¥
            absolute_url = urljoin(base_url, url)
            return f'{tag}="{absolute_url}"'

        # åŒ¹é… href å’Œ src å±æ€§
        content = re.sub(r'(href|src)="([^"]*)"', fix_html_link, content)

        # ç§»é™¤ç©ºçš„ href å±æ€§ï¼ˆä¼šå¯¼è‡´é¡µé¢è·³è½¬åˆ°è‡ªèº«ï¼‰
        content = re.sub(r'href=""', 'href="#"', content)

        return content


# ==================== å…¨å±€å®ä¾‹æ±  ====================

_browser_pool: BrowserPool | None = None


def get_browser_pool() -> BrowserPool:
    """è·å–æµè§ˆå™¨å®ä¾‹æ± ï¼ˆå•ä¾‹ï¼‰"""
    global _browser_pool
    if _browser_pool is None:
        pool_size = Config.POOL_SIZE
        _browser_pool = BrowserPool(pool_size)
    return _browser_pool


# ==================== FastAPI åº”ç”¨ ====================

app = FastAPI(
    title="Browser Fetch Service",
    description="ç‹¬ç«‹çš„ç½‘é¡µæŠ“å–æœåŠ¡ï¼Œæ”¯æŒé«˜å¹¶å‘",
    version="1.0.0"
)


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "Browser Fetch Service",
        "version": "1.0.0",
        "status": "running",
        "pool_size": Config.POOL_SIZE,
    }


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    pool = get_browser_pool()
    mem_info = get_memory_info()

    # è®¡ç®—è¿è¡Œæ—¶é—´
    uptime = time.time() - pool._start_time if pool._start_time else 0

    return {
        "status": "healthy",
        "browser_started": pool._initialized,
        "pool_size": Config.POOL_SIZE,
        "max_concurrent": Config.MAX_CONCURRENT_PAGES,
        "request_count": pool._request_count,
        "uptime_seconds": round(uptime, 2),
        "memory": mem_info,
    }


@app.get("/stats")
async def stats():
    """è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    pool = get_browser_pool()
    mem_info = get_memory_info()

    # è®¡ç®—è¿è¡Œæ—¶é—´
    uptime = time.time() - pool._start_time if pool._start_time else 0

    # ç³»ç»Ÿä¿¡æ¯
    sys_mem = psutil.virtual_memory()
    sys_cpu = psutil.cpu_percent(interval=0.1)

    return {
        "service": {
            "name": "Browser Fetch Service",
            "version": "1.0.0",
            "uptime_seconds": round(uptime, 2),
            "request_count": pool._request_count,
            "requests_per_second": round(pool._request_count / uptime, 2) if uptime > 0 else 0,
        },
        "browser_pool": {
            "pool_size": Config.POOL_SIZE,
            "max_concurrent": Config.MAX_CONCURRENT_PAGES,
            "initialized": pool._initialized,
            "active_browsers": len(pool.browsers),
        },
        "memory": {
            "process_mb": mem_info["process_rss_mb"],
            "children_mb": mem_info["children_rss_mb"],
            "total_mb": mem_info["total_rss_mb"],
            "chromium_processes": mem_info["chromium_processes"],
            "total_children": mem_info["total_children"],
        },
        "system": {
            "cpu_percent": sys_cpu,
            "memory_total_gb": round(sys_mem.total / 1024 / 1024 / 1024, 2),
            "memory_available_gb": round(sys_mem.available / 1024 / 1024 / 1024, 2),
            "memory_percent": sys_mem.percent,
        },
    }


@app.get("/metrics")
async def metrics():
    """Prometheus é£æ ¼çš„ç›‘æ§æŒ‡æ ‡"""
    pool = get_browser_pool()
    mem_info = get_memory_info()

    uptime = time.time() - pool._start_time if pool._start_time else 0

    metrics_text = f"""# HELP browser_service_requests_total Total number of requests
# TYPE browser_service_requests_total counter
browser_service_requests_total {pool._request_count}

# HELP browser_service_uptime_seconds Service uptime in seconds
# TYPE browser_service_uptime_seconds gauge
browser_service_uptime_seconds {uptime:.2f}

# HELP browser_service_pool_size Browser pool size
# TYPE browser_service_pool_size gauge
browser_service_pool_size {Config.POOL_SIZE}

# HELP browser_service_memory_bytes Total memory usage in bytes
# TYPE browser_service_memory_bytes gauge
browser_service_memory_bytes {mem_info["total_rss_mb"] * 1024 * 1024}

# HELP browser_service_chromium_processes Number of Chromium processes
# TYPE browser_service_chromium_processes gauge
browser_service_chromium_processes {mem_info["chromium_processes"]}

# HELP browser_service_max_concurrent Maximum concurrent pages per browser
# TYPE browser_service_max_concurrent gauge
browser_service_max_concurrent {Config.MAX_CONCURRENT_PAGES}
"""

    from fastapi.responses import Response
    return Response(
        content=metrics_text,
        media_type="text/plain",
    )


@app.post("/fetch_url")
async def fetch_url(
    request: FetchRequest
):
    """
    æŠ“å–ç½‘é¡µå¹¶è¿”å›å†…å®¹

    Args:
        request: æŠ“å–è¯·æ±‚

    Returns:
        åŒ…å« Markdown å†…å®¹å’Œæˆªå›¾çš„æŠ“å–ç»“æœ
    """
    pool = get_browser_pool()
    result = await pool.fetch_page(request)

    if not result.success:
        return result

    # ç›´æ¥è¿”å›å†…å­˜ä¸­çš„æ•°æ®ï¼Œä¸ç”Ÿæˆä¸´æ—¶æ–‡ä»¶
    return {
        "success": True,
        "fetched_url": result.fetched_url,
        "title": result.title,
        "markdown_content": result.content,
        "screenshot_base64": result.screenshot,
        "content_length": result.content_length,
        "fetched_at": result.fetched_at,
        "duration_seconds": result.duration_seconds
    }


# ==================== ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–æµè§ˆå™¨æ± 
    pool = get_browser_pool()
    await pool.initialize()
    logger.info("æµè§ˆå™¨æœåŠ¡å·²å°±ç»ª")

    yield

    # å…³é—­æ—¶æ¸…ç†
    await pool.shutdown()
    logger.info("æµè§ˆå™¨æœåŠ¡å·²å…³é—­")


app.router.lifespan_context = lifespan

# å¯¼å‡º app ä¾› uvicorn ä½¿ç”¨
__all__ = ["app"]
